{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Лабораторная работа: От SVM к Ансамблям деревьев\n",
    "\n",
    "\n",
    "\n",
    " **Цель:** Практическое освоение ключевых алгоритмов машинного обучения: метода опорных векторов (SVM), деревьев решений (Decision Trees) и ансамблевых методов (Ensemble Learning). Основной фокус — на использовании моделей и понимании их логики работы.\n",
    "\n",
    "\n",
    "\n",
    " **План работы:**\n",
    "\n",
    " 1.  Метод опорных векторов (SVM)\n",
    "\n",
    " 2.  Деревья решений (Decision Trees)\n",
    "\n",
    " 3.  Ансамблевые методы (Ensemble Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Часть 1: Метод опорных векторов (SVM)\n",
    "\n",
    "\n",
    "\n",
    " ### 1.1 Краткий обзор и интуиция\n",
    "\n",
    "\n",
    "\n",
    " **Принцип работы:** SVM ищет гиперплоскость, которая разделяет классы с максимальным отступом (маржой). Точки, ближайшие к этой гиперплоскости, называются опорными векторами и полностью определяют модель. Для нелинейно разделимых данных используется \"ядерный трюк\" — данные проецируются в пространство большей размерности, где они становятся линейно разделимыми.\n",
    "\n",
    "\n",
    "\n",
    " **Источники для изучения:**\n",
    "\n",
    " *   [Scikit-learn User Guide: Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    " *   [Метод опорных векторов](https://habr.com/ru/companies/ods/articles/484148/?ysclid=mgrx76r29s638032555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Генерируем данные\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60) # type: ignore\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn')\n",
    "plt.title(\"Исходные данные\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 1.1: Обучение линейного SVM**\n",
    "\n",
    "\n",
    "\n",
    " 1.  Импортируйте класс `SVC` из `sklearn.svm`.\n",
    "\n",
    " 2.  Создайте объект модели с линейным ядром (`kernel='linear'`) и параметром `C=100`.\n",
    "\n",
    " 3.  Обучите модель на данных `X`, `y`.\n",
    "\n",
    " 4.  Визуализируйте результат с помощью функции `plot_svc_decision_function` (она уже реализована ниже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: Создайте и обучите модель\n",
    "# model = ...\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, \n",
    "               linewidth=2, facecolors='none', edgecolors='black', label='Опорные векторы')\n",
    "    ax.legend()\n",
    "\n",
    "# Визуализация\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "# TODO: Раскомментируйте и вызовите функцию для вашей модели\n",
    "# plot_svc_decision_function(model)\n",
    "plt.title(\"SVM с линейным ядром\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 1.2: Анализ атрибутов модели**\n",
    "\n",
    "\n",
    "\n",
    " Изучите атрибуты обученной модели `model`. Выведите на экран:\n",
    "\n",
    " 1.  Координаты опорных векторов (`support_vectors_`).\n",
    "\n",
    " 2.  Количество опорных векторов для каждого класса (`n_support_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Выведите координаты опорных векторов\n",
    "# TODO: Выведите количество опорных векторов для каждого класса\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 1.3: Эксперименты с ядрами**\n",
    "\n",
    "\n",
    "\n",
    " **Принцип работы ядер:** Ядро — это функция, которая вычисляет скалярное произведение двух векторов в преобразованном пространстве признаков, не выполняя явного преобразования. Это позволяет эффективно работать с нелинейными зависимостями.\n",
    "\n",
    "\n",
    "\n",
    " Протестируйте разные ядра (`'linear'`, `'rbf'`, `'poly'`) на нелинейных датасетах (`make_moons`, `make_circles`). Проанализируйте, какое ядро лучше подходит для каждого типа данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "def test_kernels(X_data, y_data, title):\n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        # TODO: Создайте и обучите модель SVM с текущим ядром\n",
    "        # clf = ...\n",
    "        # clf.fit(...)\n",
    "        plt.scatter(X_data[:, 0], X_data[:, 1], c=y_data, s=30, cmap='autumn')\n",
    "        # plot_svc_decision_function(clf)\n",
    "        plt.title(f'{title}, {kernel} kernel')\n",
    "    plt.show()\n",
    "\n",
    "# Генерация и тестирование на \"Moons\"\n",
    "X_moon, y_moon = make_moons(n_samples=100, noise=0.1, random_state=0)\n",
    "test_kernels(X_moon, y_moon, \"Moons\")\n",
    "\n",
    "# Генерация и тестирование на \"Circles\"\n",
    "X_circle, y_circle = make_circles(n_samples=100, noise=0.1, factor=0.2, random_state=0)\n",
    "test_kernels(X_circle, y_circle, \"Circles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Часть 2: Деревья решений (Decision Trees)\n",
    "\n",
    "\n",
    "\n",
    " ### 2.1 Краткий обзор и интуиция\n",
    "\n",
    "\n",
    "\n",
    " **Принцип работы:** Дерево решений строится рекурсивно, разбивая набор данных на подмножества на основе значений признаков. На каждом шаге выбирается такое разбиение, которое максимизирует \"чистоту\" дочерних узлов (минимизирует неоднородность). Для измерения неоднородности используются энтропия или индекс Джини. Процесс останавливается, когда достигается условие остановки (например, максимальная глубина или минимальное количество образцов в листе).\n",
    "\n",
    "\n",
    "\n",
    " **Источники для изучения:**\n",
    "\n",
    " *   [Scikit-learn User Guide: Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    " *   [Хабр: Деревья решений — теория](https://habr.com/ru/companies/productstar/articles/523044/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target # type: ignore\n",
    "pd.DataFrame(iris.data, columns=iris.feature_names).head() # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 2.1: Обучение и визуализация дерева**\n",
    "\n",
    "\n",
    "\n",
    " 1.  Импортируйте `DecisionTreeClassifier` из `sklearn.tree`.\n",
    "\n",
    " 2.  Создайте объект дерева с критерием `'entropy'` и максимальной глубиной `3`.\n",
    "\n",
    " 3.  Обучите дерево на данных `X`, `y`.\n",
    "\n",
    " 4.  Выведите текстовое представление дерева с помощью `export_text`.\n",
    "\n",
    " 5.  Визуализируйте дерево с помощью `plot_tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Импортируйте необходимые модули\n",
    "\n",
    "# TODO: Создайте и обучите дерево\n",
    "# tree_clf = ...\n",
    "\n",
    "# TODO: Выведите текстовое представление\n",
    "# print(...)\n",
    "\n",
    "# TODO: Визуализируйте дерево\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plot_tree(...)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 2.2: Реализация метрик неоднородности**\n",
    "\n",
    "\n",
    "\n",
    " Реализуйте функции для расчета энтропии и информационного выигрыша. Это поможет понять, как дерево принимает решения о разбиении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"Рассчитывает энтропию для массива меток.\"\"\"\n",
    "    # TODO: Реализуйте функцию\n",
    "    # 1. Найдите уникальные классы и их количество с помощью np.unique(..., return_counts=True)\n",
    "    # 2. Рассчитайте вероятности для каждого класса\n",
    "    # 3. Вычислите энтропию по формуле: -sum(p * log2(p))\n",
    "    # Не забудьте обработать случай, когда p=0 (log(0) не определён)\n",
    "    pass\n",
    "\n",
    "def information_gain(parent, left_child, right_child):\n",
    "    \"\"\"Рассчитывает информационный выигрыш от разбиения.\"\"\"\n",
    "    # TODO: Реализуйте функцию\n",
    "    # IG = Entropy(parent) - (N_left/N_total * Entropy(left) + N_right/N_total * Entropy(right))\n",
    "    pass\n",
    "\n",
    "# Тест\n",
    "y_parent = np.array([0]*50 + [1]*50)\n",
    "y_left = np.array([0]*50)\n",
    "y_right = np.array([1]*50)\n",
    "\n",
    "print(f\"Энтропия родителя: {entropy(y_parent):.3f}\")\n",
    "print(f\"Информационный выигрыш: {information_gain(y_parent, y_left, y_right):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Часть 3: Ансамблевые методы (Ensemble Learning)\n",
    "\n",
    "\n",
    "\n",
    " ### 3.1 Краткий обзор и интуиция\n",
    "\n",
    "\n",
    "\n",
    " **Принцип работы:**\n",
    "\n",
    " *   **Бэггинг (Bagging):** Обучает множество моделей независимо на случайных подвыборках данных (с возвращением). Итоговое предсказание — это усреднение (регрессия) или голосование (классификация). Цель — уменьшить дисперсию модели.\n",
    "\n",
    " *   **Случайный лес (Random Forest):** Это бэггинг для деревьев решений с дополнительной случайностью: при каждом разбиении узла рассматривается только случайное подмножество признаков. Это снижает корреляцию между деревьями и улучшает обобщающую способность.\n",
    "\n",
    " *   **Бустинг (Boosting):** Обучает модели последовательно. Каждая новая модель пытается исправить ошибки предыдущих, уделяя больше внимания плохо классифицированным объектам (увеличивая их вес). Цель — уменьшить смещение модели.\n",
    "\n",
    "\n",
    "\n",
    " **Источники для изучения:**\n",
    "\n",
    " *   [Scikit-learn User Guide: Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    " *   [AdaBoost](https://neerc.ifmo.ru/wiki/index.php?title=Бустинг,_AdaBoost)\n",
    "\n",
    " *   [Ансамбли в машинном обучении](https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Загрузка и разбиение данных\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target # type: ignore\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 3.1: Сравнение одиночной модели и ансамблей**\n",
    "\n",
    "\n",
    "\n",
    " Обучите и сравните по точности:\n",
    "\n",
    " 1.  Одиночное дерево решений.\n",
    "\n",
    " 2.  Случайный лес (`RandomForestClassifier`).\n",
    "\n",
    " 3.  AdaBoost (`AdaBoostClassifier`).\n",
    "\n",
    "\n",
    "\n",
    " Проанализируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# TODO: Импортируйте RandomForestClassifier и AdaBoostClassifier\n",
    "\n",
    "# 1. Одиночное дерево\n",
    "# TODO: Создайте, обучите и оцените точность\n",
    "single_tree_acc = 0.0\n",
    "\n",
    "# 2. Случайный лес\n",
    "# TODO: Создайте, обучите и оцените точность\n",
    "# rf = \n",
    "rf_acc = 0.0\n",
    "\n",
    "# 3. AdaBoost\n",
    "# TODO: Создайте, обучите и оцените точность\n",
    "# Используйте DecisionTreeClassifier как базовый классификатор\n",
    "# ada = \n",
    "ada_acc = 0.0\n",
    "\n",
    "print(f\"Точность одиночного дерева: {single_tree_acc:.3f}\")\n",
    "print(f\"Точность случайного леса: {rf_acc:.3f}\")\n",
    "print(f\"Точность AdaBoost: {ada_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Задание 3.2: Бэггинг вручную**\n",
    "\n",
    "\n",
    "\n",
    " Реализуйте простой алгоритм бэггинга для деревьев решений. Это поможет глубже понять его принцип работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def bootstrap_sample(X, y):\n",
    "    \"\"\"Создаёт бутстрэп-выборку из данных X, y.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "n_trees = 50\n",
    "trees = []\n",
    "\n",
    "# Обучение ансамбля\n",
    "for _ in range(n_trees):\n",
    "    X_bs, y_bs = bootstrap_sample(X_train, y_train)\n",
    "    # TODO: Создайте и обучите дерево на бутстрэп-выборке\n",
    "    # tree = ...\n",
    "    # \n",
    "    trees.append(tree)\n",
    "\n",
    "# Предсказание и агрегация\n",
    "predictions = np.array([tree.predict(X_test) for tree in trees])\n",
    "mode_result = stats.mode(predictions, axis=0, keepdims=True)\n",
    "bagging_pred = mode_result.mode[0]\n",
    "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "print(f\"Точность бэггинга (вручную): {bagging_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
